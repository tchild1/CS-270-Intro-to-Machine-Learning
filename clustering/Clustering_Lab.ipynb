{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVL7_bgmIAPR"
   },
   "source": [
    "# Unsupervised Learning: Clustering Lab\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6ZbYjZZZ_yLV"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCcEPx5VIORj"
   },
   "source": [
    "## 1. Initial practice with the K-means and HAC algorithms\n",
    "\n",
    "### 1.1 (10%) K-means\n",
    "Run K-means on this [Abalone Dataset.](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/abalone.arff)\n",
    "The dataset was modified to be smaller. The last datapoint should be on line 359 or the point 0.585,0.46,0.185,0.922,0.3635,0.213,0.285,10. The remaining points are commented out. Treat the output class (last column) as an additional input feature. Create your K-Mmeans model with the paramaters K-means(n_clusters=3, init='random', n_init=1) \n",
    "\n",
    "Output the following:\n",
    "- Class label for each point (labels_)\n",
    "- The k=3 cluster centers (cluster_centers_)\n",
    "- Number of iterations it took to converge (n_iter_)\n",
    "- Total sum squared error of each point from its cluster center (inertia_)\n",
    "- The total average silhouette score (see sklearn.metrics silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  [0 2 2 1 2 2 0 0 2 0 1 1 1 1 1 1 2 1 2 2 1 1 1 2 1 1 1 1 0 1 1 0 0 0 1 2 0\n",
      " 2 1 2 2 1 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 1 2 2 2 1 1 1 2 1 2 0 1\n",
      " 1 0 2 1 1 1 2 1 0 0 1 1 1 1 2 1 0 1 1 1 0 1 2 2 2 1 2 0 0 1 1 1 1 1 2 2 2\n",
      " 2 2 2 1 1 1 1 2 2 2 2 1 2 2 2 2 2 0 0 0 2 2 2 2 2 2 2 2 2 1 1 0 1 1 1 1 2\n",
      " 2 2 0 2 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 2 2 2 2 2 2 2 2 1 1 0 1 1 1\n",
      " 1 1 1 1 2 1 1 1 2 1 1 1 0 0 1] \n",
      "\n",
      "Cluster centers:  [[ 0.61366667  0.48933333  0.16716667  1.29283333  0.48815     0.25873333\n",
      "   0.45488333 17.06666667]\n",
      " [ 0.56078652  0.44050562  0.15308989  0.94596067  0.37410112  0.20873596\n",
      "   0.30108989 11.49438202]\n",
      " [ 0.40975309  0.31709877  0.10277778  0.39740741  0.16626543  0.09267284\n",
      "   0.12505556  7.55555556]] \n",
      "\n",
      "Number of Iterations to converge:  4 \n",
      "\n",
      "Total sum squared error of each point from its cluser center:  512.9856925785199 \n",
      "\n",
      "Total average solhouette score:  0.5014855438589172 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K-means with Abalone\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# get data\n",
    "abaloneData = arff.loadarff('abalone.arff')\n",
    "\n",
    "# put data into a data frame\n",
    "abaloneDataFrame = pd.DataFrame(abaloneData[0])\n",
    "\n",
    "clf = KMeans(n_clusters=3, init='random', n_init=1)\n",
    "\n",
    "clf.fit(abaloneDataFrame)\n",
    "\n",
    "print('Labels: ', clf.labels_, '\\n')\n",
    "print('Cluster centers: ', clf.cluster_centers_, '\\n')\n",
    "print('Number of Iterations to converge: ', clf.n_iter_, '\\n')\n",
    "print('Total sum squared error of each point from its cluser center: ', clf.inertia_, '\\n')\n",
    "print('Total average solhouette score: ', silhouette_score(abaloneDataFrame, clf.labels_), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**  \n",
    "\n",
    "In the above cell, I ran a K-Means classifier with the parameters n_clusters=3, init='random', and n_init=1. The K-Means classifier, chooses n_clusters number of centroid points (in this case 3), and will group each point into the cluster of the nearest centroid point. Then, after grouping all points, the K-Means algorithm with recalculate the centroid point, then regroup all points accordingly. This process of calculating a centroid, then regrouping points into the group of the nearest centroid repeats until the centroids stop moving.  \n",
    "\n",
    "After running the K-Means classifier on this dataset, I output the following information:  \n",
    "1. Labels. First, I output the labels expected by the K-Means algorithm:  \n",
    "\n",
    "[0 2 2 1 2 2 0 0 2 0 1 1 1 1 1 1 2 1 2 2 1 1 1 2 1 1 1 1 0 1 1 0 0 0 1 2 0\n",
    " 2 1 2 2 1 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 1 2 2 2 1 1 1 2 1 2 0 1\n",
    " 1 0 2 1 1 1 2 1 0 0 1 1 1 1 2 1 0 1 1 1 0 1 2 2 2 1 2 0 0 1 1 1 1 1 2 2 2\n",
    " 2 2 2 1 1 1 1 2 2 2 2 1 2 2 2 2 2 0 0 0 2 2 2 2 2 2 2 2 2 1 1 0 1 1 1 1 2\n",
    " 2 2 0 2 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 2 2 2 2 2 2 2 2 1 1 0 1 1 1\n",
    " 1 1 1 1 2 1 1 1 2 1 1 1 0 0 1]  \n",
    "\n",
    "This list is the cluster group (0, 1, or 2) for each point.  \n",
    "\n",
    "\n",
    "2. Cluster Centers. The next thing I output from the K-Means algorithm is the Cluster Centers:  \n",
    "\n",
    "[\n",
    "    [ 0.61366667  0.48933333  0.16716667  1.29283333  0.48815     0.25873333 0.45488333 17.06666667],\n",
    "    [ 0.56078652  0.44050562  0.15308989  0.94596067  0.37410112  0.20873596 0.30108989 11.49438202],\n",
    "    [ 0.40975309  0.31709877  0.10277778  0.39740741  0.16626543  0.09267284 0.12505556  7.55555556]\n",
    "]  \n",
    "\n",
    "This output that you can see above provides the final coordinates of each cluster center. As expected, because there are 3 clusters, there are 3 centroid points output by the clf.cluster_centers_ attribute. \n",
    "\n",
    "\n",
    "3. Number of Iterations to converge: 4. This attribute, given by clf.n_iter_ tells us the number of times centroids were recalculated and instances regrouped before convergence (aka. centeroids stopped moving) occurred. In this case, it took 4 recalculations to converge.  \n",
    "\n",
    "\n",
    "4. Total sum squared error of each point from its cluser center: 512.9856925785199. This value, given by clf.inertia_, is a measure of how close each point is to its cluster center. A small value here would mean that all points are very close to their final centroid point. A large value signals that each point is very far from its centroid point.  \n",
    "\n",
    "\n",
    "5. Total average silhouette score:  0.5014855438589172. Silhouette score is an indication of 2 things. First, how closely grouped clusters are. And second, how distantly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KibCIXIThpbE"
   },
   "source": [
    "### 1.2 (10%) Hierarchical Agglomerative Clustering (HAC) \n",
    "\n",
    "Run HAC on the same [Abalone Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/abalone.arff) using complete linkage and k=3.\n",
    "\n",
    "Output the following:\n",
    "- Class label for each point (labels_)\n",
    "- The total average silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAC with Abalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vWiTdlbR2Xh"
   },
   "source": [
    "## 2. K-means Clustering with the [Iris Dataset](https://raw.githubusercontent.com/cs472ta/CS472/master/datasets/iris.arff)\n",
    "Use the Iris data set for 2.1 and 2.2.  Don't include the output label as one of the input features.\n",
    "\n",
    "### 2.1 (20%) K-means Initial Centroids Experiments\n",
    "K-means results differ based on the initial centroids used.\n",
    "- Run K-means 5 times with *k*=4, each time with different initial random centroids (init=\"random) and with n_init=1.  Give inertia and silhouette scores for each run and discuss any variations in the results.\n",
    "- SKlearn has a parameter that does this automatically (n_init).  n_init = z runs K-means z times, each with different random centroids and returns the clustering with the best SSE (intertia) of the z runs. Try it out and discuss how it does and how it compares with your 5 runs above.\n",
    "- Sklearn also has a parameter (init:'K-means++') which runs a simpler fast version of K-means first on the data to come up with good initial centroids, and then runs regular K-means with this centroids.  Try it out (with n_init = 1) and discuss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SSoasDQSKXb"
   },
   "outputs": [],
   "source": [
    "# K-means initial centroid experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjkatnQY-Jep"
   },
   "source": [
    "Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGfrL7p5-Jeq"
   },
   "source": [
    "### 2.2 (20%) Silhouette Graphs\n",
    "In this part you will show silhouette graphs for different *k* values.  Install the [Yellowbrick visualization package](https://www.scikit-yb.org/en/latest/quickstart.html) and import the [Silhouette Visualizer](https://www.scikit-yb.org/en/latest/api/cluster/silhouette.html).  This library includes lots of visualization packages which you might find useful. (Note: The YellowBrick silhouette visualizer does not currently support HAC).\n",
    "- Show Silhouette graphs for clusterings with *k* = 2-6. Print the SSE (inertia) and total silhouette score for each.\n",
    "- Learn with the default n_init = 10 to help insure a decent clustering.\n",
    "- Using the silhouette graphs, choose which *k* you think is best and discuss why. Think about and discuss more than just the total silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gigfanaW-Jeq"
   },
   "outputs": [],
   "source": [
    "# Iris Clustering with K-means and silhouette graphs\n",
    "from yellowbrick.cluster import SilhouetteVisualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IELq_zlu-Jeq"
   },
   "source": [
    "Discuss your results and justify which clustering is best based on the silhouette graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 (20%) Iris Clustering with HAC\n",
    "\n",
    "- Use the same dataset as above and learn with HAC clustering\n",
    "- Create one table with silhouette scores for k=2-6 for each of the linkage options single, average, complete, and ward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HAC with Iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion and linkage comparison*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBBmeNQ7jvcQ"
   },
   "source": [
    "## 4 (20%) Run both algorithms on a real world data\n",
    "- Choose any real world data set which you have not used previously\n",
    "- Use parameters of your choosing\n",
    "- Try each algorithm a few times with different parameters and output one typical example of labels and silhouette scores for each algorithm\n",
    "- Show the silhouette graph for at least one reasonable *k* value for K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFQv70W2VyqJ"
   },
   "outputs": [],
   "source": [
    "# Run both algoriths on a data set of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-oKHPPT-Jer"
   },
   "source": [
    "*Discussion and comparison*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extra Credit for Coding Your Own Clustering Algorithms\n",
    "### 5.1 (Optional 10% extra credit) Code up the K-means clustering algorithm \n",
    "Below is a scaffold you could use if you want. As above, you only need to support numeric inputs, but think about how you would support nominal inputs and unknown values. Requirements for this task:\n",
    "- Your model should support the methods shown in the example scaffold below.\n",
    "- Ability to choose *k* and specify the *k* initial centroids.\n",
    "- Run and show the cluster label for each point with both the Iris data set and the data set of your choice above.\n",
    "\n",
    "### 5.2 (Optional 10% extra credit) Code up the HAC clustering algorithm \n",
    "\n",
    "- Your model should support the methods shown in the example scaffold below.\n",
    "- HAC should support both single link and complete link options.\n",
    "- HAC automatically generates all clusterings from *n* to 2.  You just need to output results for the curent chosen *k*.\n",
    "- Run and show the cluster label for each point with both the Iris data set and the data set of your choice above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion and comparision of each model implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin, ClusterMixin\n",
    "\n",
    "class KMEANSClustering(BaseEstimator,ClusterMixin):\n",
    "\n",
    "    def __init__(self,k=3,debug=False): ## add parameters here\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            k = how many final clusters to have\n",
    "            debug = if debug is true use the first k instances as the initial centroids otherwise choose random points as the initial centroids.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.debug = debug\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" Fit the data; In this lab this will make the K clusters :D\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data\n",
    "            y (array-like): An optional argument. Clustering is usually unsupervised so you don't need labels\n",
    "        Returns:\n",
    "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def print_labels(self): # Print the cluster label for each data point\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HACClustering(BaseEstimator,ClusterMixin):\n",
    "\n",
    "    def __init__(self,k=3,link_type='single'): ## add parameters here\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            k = how many final clusters to have\n",
    "            link_type = single or complete. when combining two clusters use complete link or single link\n",
    "        \"\"\"\n",
    "        self.link_type = link_type\n",
    "        self.k = k\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" Fit the data; In this lab this will make the K clusters :D\n",
    "        Args:\n",
    "            X (array-like): A 2D numpy array with the training data\n",
    "            y (array-like): An optional argument. Clustering is usually unsupervised so you don't need labels\n",
    "        Returns:\n",
    "            self: this allows this to be chained, e.g. model.fit(X,y).predict(X_test)\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def print_labels(self): # Print the cluster label for each data point\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
